{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***(Post-Training Quantization)***\n",
        "\n",
        "\n",
        "\n",
        "Этот метод применяется к уже обученной модели без необходимости переобучения."
      ],
      "metadata": {
        "id": "3C2iO2o8G4Gw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knli-ku8Gu8Z",
        "outputId": "49c8805b-b6e1-4e66-f704-c095504ae206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleModel(\n",
            "  (fc1): Linear(\n",
            "    in_features=784, out_features=256, bias=True\n",
            "    (activation_post_process): MinMaxObserver(min_val=-1.8477951288223267, max_val=1.6896419525146484)\n",
            "  )\n",
            "  (fc2): Linear(\n",
            "    in_features=256, out_features=10, bias=True\n",
            "    (activation_post_process): MinMaxObserver(min_val=-0.1601126790046692, max_val=0.3551177382469177)\n",
            "  )\n",
            ")\n",
            "SimpleModel(\n",
            "  (fc1): QuantizedLinear(in_features=784, out_features=256, scale=0.02785383351147175, zero_point=66, qscheme=torch.per_tensor_affine)\n",
            "  (fc2): QuantizedLinear(in_features=256, out_features=10, scale=0.004056932404637337, zero_point=39, qscheme=torch.per_tensor_affine)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "\n",
        "# Пример простой модели\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Создаем модель\n",
        "model = SimpleModel()\n",
        "\n",
        "# Убедимся, что модель в режиме оценки\n",
        "model.eval()\n",
        "\n",
        "# Пример данных\n",
        "input_fp32 = torch.randn(1, 784)\n",
        "\n",
        "# Квантование модели\n",
        "model.qconfig = torch.quantization.default_qconfig\n",
        "torch.quantization.prepare(model, inplace=True)\n",
        "\n",
        "# Калибровка модели (необходимо для посттренировочного квантования)\n",
        "# Здесь можно использовать реальные данные для калибровки\n",
        "with torch.no_grad():\n",
        "    model(input_fp32)\n",
        "\n",
        "# Применяем квантование\n",
        "model_quantized = torch.quantization.convert(model, inplace=False)\n",
        "print(model)\n",
        "# Теперь модель квантована\n",
        "print(model_quantized)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***(Quantization Aware Training)***\n",
        "\n",
        "\n",
        "Этот метод предполагает обучение модели с учетом квантования, что может улучшить точность квантованной модели."
      ],
      "metadata": {
        "id": "Yaf5wF5wHOXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "\n",
        "# Пример простой модели\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Создаем модель\n",
        "model = SimpleModel()\n",
        "\n",
        "# Убедимся, что модель в режиме обучения\n",
        "model.train()\n",
        "\n",
        "# Добавляем квантование с осознанием обучения\n",
        "model.qconfig = torch.quantization.default_qat_qconfig\n",
        "torch.quantization.prepare_qat(model, inplace=True)\n",
        "\n",
        "# Обучаем модель (здесь должен быть ваш код обучения)\n",
        "# ...\n",
        "\n",
        "# После обучения, конвертируем модель в квантованную\n",
        "model.eval()\n",
        "model_quantized = torch.quantization.convert(model, inplace=False)\n",
        "\n",
        "# Теперь модель квантована\n",
        "print(model_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0do7wBNZHNWw",
        "outputId": "b82308f0-2f7c-4651-c9a8-dbc1f9389e9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleModel(\n",
            "  (fc1): QuantizedLinear(in_features=784, out_features=256, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            "  (fc2): QuantizedLinear(in_features=256, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/utils.py:407: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Квантование с использованием torch.quantization.quantize_dynamic***\n",
        "\n",
        "\n",
        "\n",
        "Этот метод позволяет динамически квантовать модель, что особенно полезно для моделей с большим количеством линейных слоев."
      ],
      "metadata": {
        "id": "aHnjS_-iH6vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Пример простой модели\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Создаем модель\n",
        "model = SimpleModel()\n",
        "\n",
        "# Динамическое квантование\n",
        "model_quantized = torch.quantization.quantize_dynamic(\n",
        "    model, {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Теперь модель квантована\n",
        "print(model_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A47vbLKzIAN5",
        "outputId": "ad89e799-3a3c-4f17-e5e0-1e652de34427"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleModel(\n",
            "  (fc1): DynamicQuantizedLinear(in_features=784, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "  (fc2): DynamicQuantizedLinear(in_features=256, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}