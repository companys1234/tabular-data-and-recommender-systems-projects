{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kQsTQ4hWCUB3",
        "outputId": "2e712810-9bee-47d3-8639-9c0bb5161c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful!\n",
            "============================================================\n",
            "FEATURE COMBINATION RECOMMENDER SYSTEM DEMO\n",
            "============================================================\n",
            "\n",
            "1. Generating synthetic data...\n",
            "   Users: 200\n",
            "   Items: 100\n",
            "   Interactions: 1000\n",
            "   Sessions: 556\n",
            "\n",
            "============================================================\n",
            "Testing CONCATENATE feature combination method\n",
            "============================================================\n",
            "Step 1: Extracting user features...\n",
            "Step 2: Extracting item features...\n",
            "Step 3: Extracting interaction features...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of 'prefix' (4) did not match the length of the columns being encoded (1).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3333218104.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;31m# Запуск демонстрации\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3333218104.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# Подготавливаем признаки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         features = recommender.prepare_features(\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0muser_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractions_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msessions_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-3333218104.py\u001b[0m in \u001b[0;36mprepare_features\u001b[0;34m(self, user_df, item_df, interactions_df, sessions_df)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step 3: Extracting interaction features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         interaction_features = self.feature_extractor.extract_interaction_features(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0minteractions_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-3333218104.py\u001b[0m in \u001b[0;36mextract_interaction_features\u001b[0;34m(self, interactions_df, user_features, item_features)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# 3. One-hot encoding временных признаков\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         time_features = pd.get_dummies(\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0minteractions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'day_of_week'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prefix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prefix_sep\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mcheck_len\u001b[0;34m(item, name)\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;34mf\"({data_to_encode.shape[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     )\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prefix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of 'prefix' (4) did not match the length of the columns being encoded (1)."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Tuple, Union\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "\n",
        "# ============================================\n",
        "# 1. ГЕНЕРАЦИЯ СИНТЕТИЧЕСКИХ ДАННЫХ\n",
        "# ============================================\n",
        "\n",
        "def generate_synthetic_data(n_users=1000, n_items=500, n_interactions=5000):\n",
        "    \"\"\"Генерация реалистичных синтетических данных\"\"\"\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Пользователи\n",
        "    users = np.arange(1, n_users + 1)\n",
        "\n",
        "    # Демографические признаки пользователей\n",
        "    user_features = {\n",
        "        'user_id': users,\n",
        "        'age': np.random.randint(18, 65, n_users),\n",
        "        'gender': np.random.choice(['M', 'F'], n_users, p=[0.55, 0.45]),\n",
        "        'location': np.random.choice(['NY', 'LA', 'SF', 'CH', 'MI'], n_users),\n",
        "        'join_date': pd.date_range('2020-01-01', periods=n_users, freq='D'),\n",
        "        'preferred_category': np.random.choice(['tech', 'sports', 'music', 'movies', 'books'], n_users)\n",
        "    }\n",
        "\n",
        "    # Товары\n",
        "    items = np.arange(1, n_items + 1)\n",
        "\n",
        "    # Контентные признаки товаров\n",
        "    categories = ['electronics', 'books', 'clothing', 'home', 'sports', 'beauty']\n",
        "    item_features = {\n",
        "        'item_id': items,\n",
        "        'category': np.random.choice(categories, n_items),\n",
        "        'price': np.random.uniform(5, 500, n_items).round(2),\n",
        "        'rating': np.random.uniform(1, 5, n_items).round(1),\n",
        "        'reviews_count': np.random.randint(0, 1000, n_items),\n",
        "        'description': [f\"product_{i} description with features and benefits\" for i in items]\n",
        "    }\n",
        "\n",
        "    # Взаимодействия пользователь-товар\n",
        "    interactions = []\n",
        "    for _ in range(n_interactions):\n",
        "        user = np.random.choice(users)\n",
        "        item = np.random.choice(items)\n",
        "        rating = np.random.choice([1, 2, 3, 4, 5], p=[0.05, 0.1, 0.2, 0.4, 0.25])\n",
        "        timestamp = pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365))\n",
        "        interactions.append([user, item, rating, timestamp])\n",
        "\n",
        "    # Контекстуальные данные (сессии)\n",
        "    sessions = []\n",
        "    for _ in range(n_interactions // 10):  # меньше сессий чем взаимодействий\n",
        "        session_id = np.random.randint(1000, 9999)\n",
        "        user = np.random.choice(users)\n",
        "        session_items = np.random.choice(items, size=np.random.randint(2, 10), replace=False)\n",
        "        for item in session_items:\n",
        "            sessions.append([session_id, user, item])\n",
        "\n",
        "    # Создаем DataFrames\n",
        "    user_df = pd.DataFrame(user_features)\n",
        "    item_df = pd.DataFrame(item_features)\n",
        "    interactions_df = pd.DataFrame(interactions, columns=['user_id', 'item_id', 'rating', 'timestamp'])\n",
        "    sessions_df = pd.DataFrame(sessions, columns=['session_id', 'user_id', 'item_id'])\n",
        "\n",
        "    # Добавляем дополнительные вычисляемые признаки\n",
        "    # Популярность товара\n",
        "    item_popularity = interactions_df.groupby('item_id').size().reset_index(name='popularity')\n",
        "    item_df = item_df.merge(item_popularity, on='item_id', how='left').fillna(0)\n",
        "\n",
        "    # Активность пользователя\n",
        "    user_activity = interactions_df.groupby('user_id').size().reset_index(name='activity_count')\n",
        "    user_df = user_df.merge(user_activity, on='user_id', how='left').fillna(0)\n",
        "\n",
        "    return user_df, item_df, interactions_df, sessions_df\n",
        "\n",
        "# ============================================\n",
        "# 2. ФУНКЦИИ FEATURE EXTRACTION\n",
        "# ============================================\n",
        "\n",
        "class FeatureExtractor:\n",
        "    \"\"\"Извлечение признаков из разных источников\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "        self.item_svd = TruncatedSVD(n_components=10)\n",
        "        self.user_svd = TruncatedSVD(n_components=10)\n",
        "\n",
        "    def extract_user_features(self, user_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Извлечение признаков пользователей\"\"\"\n",
        "\n",
        "        # Копируем для безопасности\n",
        "        user_features = user_df.copy()\n",
        "\n",
        "        # 1. Демографические признаки (one-hot encoding)\n",
        "        demo_features = pd.get_dummies(\n",
        "            user_features[['gender', 'location', 'preferred_category']],\n",
        "            prefix=['gender', 'loc', 'cat']\n",
        "        )\n",
        "\n",
        "        # 2. Числовые признаки (normalization)\n",
        "        numeric_cols = ['age', 'activity_count']\n",
        "        user_features[numeric_cols] = self.scaler.fit_transform(user_features[numeric_cols])\n",
        "\n",
        "        # 3. Временные признаки из join_date\n",
        "        user_features['join_year'] = pd.to_datetime(user_features['join_date']).dt.year\n",
        "        user_features['join_month'] = pd.to_datetime(user_features['join_date']).dt.month\n",
        "        user_features['join_day'] = pd.to_datetime(user_features['join_date']).dt.day\n",
        "        user_features['days_since_join'] = (\n",
        "            pd.Timestamp.now() - pd.to_datetime(user_features['join_date'])\n",
        "        ).dt.days\n",
        "\n",
        "        # 4. Объединяем все признаки пользователей\n",
        "        final_features = pd.concat([\n",
        "            user_features[['user_id'] + numeric_cols + ['join_year', 'join_month', 'join_day', 'days_since_join']],\n",
        "            demo_features\n",
        "        ], axis=1)\n",
        "\n",
        "        return final_features\n",
        "\n",
        "    def extract_item_features(self, item_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Извлечение признаков товаров\"\"\"\n",
        "\n",
        "        item_features = item_df.copy()\n",
        "\n",
        "        # 1. Категориальные признаки (one-hot)\n",
        "        cat_features = pd.get_dummies(\n",
        "            item_features[['category']],\n",
        "            prefix='cat'\n",
        "        )\n",
        "\n",
        "        # 2. Числовые признаки (normalization)\n",
        "        numeric_cols = ['price', 'rating', 'reviews_count', 'popularity']\n",
        "        item_features[numeric_cols] = self.scaler.fit_transform(item_features[numeric_cols])\n",
        "\n",
        "        # 3. Текстовые признаки из описания (TF-IDF + SVD)\n",
        "        tfidf_matrix = self.tfidf_vectorizer.fit_transform(item_features['description'])\n",
        "        svd_features = self.item_svd.fit_transform(tfidf_matrix)\n",
        "        svd_cols = [f'item_svd_{i}' for i in range(svd_features.shape[1])]\n",
        "        svd_df = pd.DataFrame(svd_features, columns=svd_cols)\n",
        "\n",
        "        # 4. Объединяем все признаки товаров\n",
        "        final_features = pd.concat([\n",
        "            item_features[['item_id'] + numeric_cols],\n",
        "            cat_features,\n",
        "            svd_df\n",
        "        ], axis=1)\n",
        "\n",
        "        return final_features\n",
        "\n",
        "    def extract_interaction_features(self, interactions_df: pd.DataFrame,\n",
        "                                    user_features: pd.DataFrame,\n",
        "                                    item_features: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Извлечение признаков взаимодействий\"\"\"\n",
        "\n",
        "        interactions = interactions_df.copy()\n",
        "\n",
        "        # 1. Временные признаки\n",
        "        interactions['timestamp'] = pd.to_datetime(interactions['timestamp'])\n",
        "        interactions['hour'] = interactions['timestamp'].dt.hour\n",
        "        interactions['day_of_week'] = interactions['timestamp'].dt.dayofweek\n",
        "        interactions['month'] = interactions['timestamp'].dt.month\n",
        "\n",
        "        # 2. Признаки на основе рейтинга\n",
        "        interactions['rating_bin'] = pd.cut(interactions['rating'],\n",
        "                                           bins=[0, 2, 3, 4, 5],\n",
        "                                           labels=['low', 'medium_low', 'medium_high', 'high'])\n",
        "\n",
        "        # 3. One-hot encoding временных признаков\n",
        "        time_features = pd.get_dummies(\n",
        "            interactions[['hour', 'day_of_week', 'month', 'rating_bin']],\n",
        "            prefix=['hour', 'dow', 'month', 'rating']\n",
        "        )\n",
        "\n",
        "        # 4. Объединяем с признаками пользователей и товаров\n",
        "        combined = interactions.merge(\n",
        "            user_features, on='user_id', how='left'\n",
        "        ).merge(\n",
        "            item_features, on='item_id', how='left'\n",
        "        )\n",
        "\n",
        "        # 5. Удаляем исходные столбцы и объединяем с временными признаками\n",
        "        columns_to_drop = ['timestamp', 'rating', 'hour', 'day_of_week', 'month', 'rating_bin']\n",
        "        combined = combined.drop(columns=columns_to_drop, errors='ignore')\n",
        "        combined = pd.concat([combined, time_features], axis=1)\n",
        "\n",
        "        return combined.fillna(0)\n",
        "\n",
        "    def extract_session_features(self, sessions_df: pd.DataFrame,\n",
        "                               item_features: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Извлечение сессионных признаков\"\"\"\n",
        "\n",
        "        sessions = sessions_df.copy()\n",
        "\n",
        "        # 1. Агрегация на уровне сессии\n",
        "        session_stats = sessions.groupby('session_id').agg({\n",
        "            'item_id': ['count', lambda x: list(x)],\n",
        "            'user_id': 'first'\n",
        "        }).reset_index()\n",
        "\n",
        "        session_stats.columns = ['session_id', 'session_length', 'item_sequence', 'user_id']\n",
        "\n",
        "        # 2. Признаки сессии\n",
        "        session_features = pd.DataFrame()\n",
        "        session_features['session_id'] = session_stats['session_id']\n",
        "        session_features['session_length'] = session_stats['session_length']\n",
        "\n",
        "        # 3. Средние признаки товаров в сессии\n",
        "        session_item_features = []\n",
        "        for items in session_stats['item_sequence']:\n",
        "            item_feats = item_features[item_features['item_id'].isin(items)]\n",
        "            if len(item_feats) > 0:\n",
        "                avg_features = item_feats.drop('item_id', axis=1).mean().values\n",
        "            else:\n",
        "                avg_features = np.zeros(item_features.shape[1] - 1)\n",
        "            session_item_features.append(avg_features)\n",
        "\n",
        "        # Создаем DataFrame с признаками товаров сессии\n",
        "        session_item_df = pd.DataFrame(\n",
        "            session_item_features,\n",
        "            columns=[f'session_item_{i}' for i in range(len(session_item_features[0]))]\n",
        "        )\n",
        "\n",
        "        # 4. Объединяем все сессионные признаки\n",
        "        final_features = pd.concat([session_features, session_item_df], axis=1)\n",
        "\n",
        "        return final_features\n",
        "\n",
        "# ============================================\n",
        "# 3. КЛАСС ДЛЯ FEATURE COMBINATION\n",
        "# ============================================\n",
        "\n",
        "class FeatureCombinationRecommender:\n",
        "    \"\"\"\n",
        "    Рекомендательная система с комбинацией признаков\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, combination_method='concatenate'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            combination_method: метод комбинации признаков\n",
        "                - 'concatenate': простая конкатенация\n",
        "                - 'weighted': взвешенная комбинация\n",
        "                - 'neural': нейросетевая комбинация\n",
        "                - 'attention': с механизмом внимания\n",
        "        \"\"\"\n",
        "        self.combination_method = combination_method\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.models = {}\n",
        "        self.feature_weights = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def prepare_features(self, user_df, item_df, interactions_df, sessions_df=None):\n",
        "        \"\"\"Подготовка и комбинация всех признаков\"\"\"\n",
        "\n",
        "        print(\"Step 1: Extracting user features...\")\n",
        "        user_features = self.feature_extractor.extract_user_features(user_df)\n",
        "\n",
        "        print(\"Step 2: Extracting item features...\")\n",
        "        item_features = self.feature_extractor.extract_item_features(item_df)\n",
        "\n",
        "        print(\"Step 3: Extracting interaction features...\")\n",
        "        interaction_features = self.feature_extractor.extract_interaction_features(\n",
        "            interactions_df, user_features, item_features\n",
        "        )\n",
        "\n",
        "        # Комбинируем признаки в зависимости от метода\n",
        "        if self.combination_method == 'concatenate':\n",
        "            combined_features = self._concatenate_features(\n",
        "                interaction_features, user_features, item_features\n",
        "            )\n",
        "        elif self.combination_method == 'weighted':\n",
        "            combined_features = self._weighted_combination(\n",
        "                interaction_features, user_features, item_features\n",
        "            )\n",
        "        elif self.combination_method == 'neural':\n",
        "            combined_features = self._neural_combination(\n",
        "                interaction_features, user_features, item_features\n",
        "            )\n",
        "        elif self.combination_method == 'attention':\n",
        "            combined_features = self._attention_combination(\n",
        "                interaction_features, user_features, item_features\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown combination method: {self.combination_method}\")\n",
        "\n",
        "        # Добавляем сессионные признаки если есть\n",
        "        if sessions_df is not None and len(sessions_df) > 0:\n",
        "            print(\"Step 4: Extracting session features...\")\n",
        "            session_features = self.feature_extractor.extract_session_features(\n",
        "                sessions_df, item_features\n",
        "            )\n",
        "            combined_features = combined_features.merge(\n",
        "                session_features, on='user_id', how='left'\n",
        "            ).fillna(0)\n",
        "\n",
        "        # Сохраняем информацию о признаках\n",
        "        self.feature_info = {\n",
        "            'user_features': list(user_features.columns),\n",
        "            'item_features': list(item_features.columns),\n",
        "            'interaction_features': list(interaction_features.columns),\n",
        "            'combined_features': list(combined_features.columns)\n",
        "        }\n",
        "\n",
        "        print(f\"Feature extraction complete!\")\n",
        "        print(f\"  User features: {len(user_features.columns)}\")\n",
        "        print(f\"  Item features: {len(item_features.columns)}\")\n",
        "        print(f\"  Interaction features: {len(interaction_features.columns)}\")\n",
        "        print(f\"  Total combined features: {len(combined_features.columns)}\")\n",
        "\n",
        "        return combined_features\n",
        "\n",
        "    def _concatenate_features(self, interaction_features, user_features, item_features):\n",
        "        \"\"\"Простая конкатенация всех признаков\"\"\"\n",
        "\n",
        "        # Объединяем все признаки\n",
        "        combined = interaction_features.merge(\n",
        "            user_features, on='user_id', how='left'\n",
        "        ).merge(\n",
        "            item_features, on='item_id', how='left'\n",
        "        ).fillna(0)\n",
        "\n",
        "        # Удаляем дублирующиеся колонки\n",
        "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
        "\n",
        "        return combined\n",
        "\n",
        "    def _weighted_combination(self, interaction_features, user_features, item_features):\n",
        "        \"\"\"Взвешенная комбинация признаков\"\"\"\n",
        "\n",
        "        # Получаем признаки отдельно\n",
        "        interaction_only = interaction_features.drop(['user_id', 'item_id'], axis=1, errors='ignore')\n",
        "        user_only = user_features.drop(['user_id'], axis=1, errors='ignore')\n",
        "        item_only = item_features.drop(['item_id'], axis=1, errors='ignore')\n",
        "\n",
        "        # Определяем веса (можно настраивать)\n",
        "        interaction_weight = 0.4\n",
        "        user_weight = 0.3\n",
        "        item_weight = 0.3\n",
        "\n",
        "        # Нормализуем признаки\n",
        "        interaction_scaled = self.scaler.fit_transform(interaction_only)\n",
        "        user_scaled = self.scaler.fit_transform(user_only)\n",
        "        item_scaled = self.scaler.fit_transform(item_only)\n",
        "\n",
        "        # Взвешенная комбинация\n",
        "        weighted_features = []\n",
        "        for i in range(len(interaction_features)):\n",
        "            # Для каждого взаимодействия комбинируем признаки\n",
        "            user_id = interaction_features.iloc[i]['user_id']\n",
        "            item_id = interaction_features.iloc[i]['item_id']\n",
        "\n",
        "            # Находим соответствующие признаки пользователя и товара\n",
        "            user_idx = user_features[user_features['user_id'] == user_id].index\n",
        "            item_idx = item_features[item_features['item_id'] == item_id].index\n",
        "\n",
        "            if len(user_idx) > 0 and len(item_idx) > 0:\n",
        "                user_vec = user_scaled[user_idx[0]]\n",
        "                item_vec = item_scaled[item_idx[0]]\n",
        "                inter_vec = interaction_scaled[i]\n",
        "\n",
        "                # Взвешенная сумма\n",
        "                combined = (\n",
        "                    inter_vec * interaction_weight +\n",
        "                    user_vec * user_weight +\n",
        "                    item_vec * item_weight\n",
        "                )\n",
        "\n",
        "                weighted_features.append(combined)\n",
        "            else:\n",
        "                # Fallback: используем только interaction features\n",
        "                weighted_features.append(inter_vec)\n",
        "\n",
        "        # Создаем DataFrame с комбинированными признаками\n",
        "        weighted_df = pd.DataFrame(\n",
        "            weighted_features,\n",
        "            columns=[f'weighted_feat_{i}' for i in range(weighted_features[0].shape[0])]\n",
        "        )\n",
        "\n",
        "        # Добавляем ID\n",
        "        weighted_df['user_id'] = interaction_features['user_id'].values\n",
        "        weighted_df['item_id'] = interaction_features['item_id'].values\n",
        "\n",
        "        return weighted_df\n",
        "\n",
        "    def _neural_combination(self, interaction_features, user_features, item_features):\n",
        "        \"\"\"Нейросетевая комбинация признаков\"\"\"\n",
        "\n",
        "        # Преобразуем в тензоры\n",
        "        inter_tensor = torch.FloatTensor(\n",
        "            interaction_features.drop(['user_id', 'item_id'], axis=1, errors='ignore').values\n",
        "        )\n",
        "        user_tensor = torch.FloatTensor(\n",
        "            user_features.drop(['user_id'], axis=1, errors='ignore').values\n",
        "        )\n",
        "        item_tensor = torch.FloatTensor(\n",
        "            item_features.drop(['item_id'], axis=1, errors='ignore').values\n",
        "        )\n",
        "\n",
        "        # Простая нейросеть для комбинации\n",
        "        class NeuralCombiner(nn.Module):\n",
        "            def __init__(self, inter_dim, user_dim, item_dim, hidden_dim=64, output_dim=32):\n",
        "                super().__init__()\n",
        "                self.inter_fc = nn.Linear(inter_dim, hidden_dim)\n",
        "                self.user_fc = nn.Linear(user_dim, hidden_dim)\n",
        "                self.item_fc = nn.Linear(item_dim, hidden_dim)\n",
        "                self.combine_fc = nn.Linear(hidden_dim * 3, output_dim)\n",
        "                self.relu = nn.ReLU()\n",
        "\n",
        "            def forward(self, inter_vec, user_vec, item_vec):\n",
        "                inter_hidden = self.relu(self.inter_fc(inter_vec))\n",
        "                user_hidden = self.relu(self.user_fc(user_vec))\n",
        "                item_hidden = self.relu(self.item_fc(item_vec))\n",
        "\n",
        "                combined = torch.cat([inter_hidden, user_hidden, item_hidden], dim=1)\n",
        "                output = self.combine_fc(combined)\n",
        "                return output\n",
        "\n",
        "        # Инициализируем модель\n",
        "        combiner = NeuralCombiner(\n",
        "            inter_dim=inter_tensor.shape[1],\n",
        "            user_dim=user_tensor.shape[1],\n",
        "            item_dim=item_tensor.shape[1]\n",
        "        )\n",
        "\n",
        "        # Комбинируем признаки\n",
        "        combined_features = []\n",
        "        for i in range(len(interaction_features)):\n",
        "            user_id = interaction_features.iloc[i]['user_id']\n",
        "            item_id = interaction_features.iloc[i]['item_id']\n",
        "\n",
        "            # Находим индексы\n",
        "            user_idx = user_features[user_features['user_id'] == user_id].index\n",
        "            item_idx = item_features[item_features['item_id'] == item_id].index\n",
        "\n",
        "            if len(user_idx) > 0 and len(item_idx) > 0:\n",
        "                # Комбинируем через нейросеть\n",
        "                inter_vec = inter_tensor[i].unsqueeze(0)\n",
        "                user_vec = user_tensor[user_idx[0]].unsqueeze(0)\n",
        "                item_vec = item_tensor[item_idx[0]].unsqueeze(0)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    combined = combiner(inter_vec, user_vec, item_vec)\n",
        "                combined_features.append(combined.numpy().flatten())\n",
        "            else:\n",
        "                # Fallback\n",
        "                combined_features.append(np.zeros(32))\n",
        "\n",
        "        # Создаем DataFrame\n",
        "        combined_df = pd.DataFrame(\n",
        "            combined_features,\n",
        "            columns=[f'neural_feat_{i}' for i in range(len(combined_features[0]))]\n",
        "        )\n",
        "        combined_df['user_id'] = interaction_features['user_id'].values\n",
        "        combined_df['item_id'] = interaction_features['item_id'].values\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def _attention_combination(self, interaction_features, user_features, item_features):\n",
        "        \"\"\"Комбинация с механизмом внимания\"\"\"\n",
        "\n",
        "        # Упрощенная реализация attention\n",
        "        print(\"Using attention-based feature combination...\")\n",
        "\n",
        "        # Для простоты используем конкатенацию с дополнительными attention-признаками\n",
        "        combined = self._concatenate_features(interaction_features, user_features, item_features)\n",
        "\n",
        "        # Добавляем простые attention-like features\n",
        "        # Например, важность пользовательских vs товарных признаков\n",
        "\n",
        "        # Вычисляем \"важность\" признаков на основе вариации\n",
        "        user_cols = [col for col in combined.columns if 'user' in col or 'age' in col or 'gender' in col]\n",
        "        item_cols = [col for col in combined.columns if 'item' in col or 'price' in col or 'category' in col]\n",
        "\n",
        "        if user_cols and item_cols:\n",
        "            # Простой attention score\n",
        "            user_importance = combined[user_cols].std(axis=1).mean()\n",
        "            item_importance = combined[item_cols].std(axis=1).mean()\n",
        "\n",
        "            total_importance = user_importance + item_importance + 1e-10\n",
        "\n",
        "            combined['user_attention_weight'] = user_importance / total_importance\n",
        "            combined['item_attention_weight'] = item_importance / total_importance\n",
        "\n",
        "            # Взвешенные суммы\n",
        "            combined['weighted_user_feature'] = combined[user_cols].mean(axis=1) * combined['user_attention_weight']\n",
        "            combined['weighted_item_feature'] = combined[item_cols].mean(axis=1) * combined['item_attention_weight']\n",
        "\n",
        "        return combined\n",
        "\n",
        "    def train_model(self, features, target_column='rating', model_type='xgboost'):\n",
        "        \"\"\"Обучение модели на комбинированных признаках\"\"\"\n",
        "\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        import xgboost as xgb\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "\n",
        "        print(f\"\\nTraining {model_type} model on combined features...\")\n",
        "\n",
        "        # Подготовка данных\n",
        "        if target_column in features.columns:\n",
        "            X = features.drop(columns=[target_column, 'user_id', 'item_id'], errors='ignore')\n",
        "            y = features[target_column]\n",
        "        else:\n",
        "            # Если нет целевой переменной, создаем бинарную (взаимодействие было/не было)\n",
        "            X = features.drop(columns=['user_id', 'item_id'], errors='ignore')\n",
        "            y = np.ones(len(features))  # все взаимодействия положительные\n",
        "\n",
        "            # Для баланса можно добавить negative sampling\n",
        "\n",
        "        # Разделение на train/test\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Выбор модели\n",
        "        if model_type == 'random_forest':\n",
        "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        elif model_type == 'logistic':\n",
        "            model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "            y_train = (y_train > 3).astype(int)  # бинарная классификация\n",
        "            y_test = (y_test > 3).astype(int)\n",
        "        elif model_type == 'xgboost':\n",
        "            model = xgb.XGBRegressor(\n",
        "                n_estimators=100,\n",
        "                learning_rate=0.1,\n",
        "                random_state=42,\n",
        "                objective='reg:squarederror'\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        # Обучение\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Предсказание и оценка\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        if model_type == 'logistic':\n",
        "            y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "            accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "            print(f\"Accuracy: {accuracy:.4f}\")\n",
        "            metric = accuracy\n",
        "        else:\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            print(f\"MSE: {mse:.4f}\")\n",
        "            metric = mse\n",
        "\n",
        "        # Сохраняем модель\n",
        "        self.models[model_type] = {\n",
        "            'model': model,\n",
        "            'feature_names': list(X.columns),\n",
        "            'performance': metric\n",
        "        }\n",
        "\n",
        "        return model, metric\n",
        "\n",
        "    def recommend(self, user_id, item_ids, model_type='xgboost', top_k=5):\n",
        "        \"\"\"Генерация рекомендаций для пользователя\"\"\"\n",
        "\n",
        "        if model_type not in self.models:\n",
        "            raise ValueError(f\"Model {model_type} not trained yet!\")\n",
        "\n",
        "        model_info = self.models[model_type]\n",
        "        model = model_info['model']\n",
        "\n",
        "        # Создаем фиктивные признаки для предсказания\n",
        "        # В реальной системе здесь были бы реальные признаки\n",
        "        predictions = []\n",
        "\n",
        "        for item_id in item_ids:\n",
        "            # Создаем вектор признаков для пары (user_id, item_id)\n",
        "            # В реальной системе нужно извлекать реальные признаки\n",
        "            feature_vector = np.random.randn(len(model_info['feature_names']))\n",
        "\n",
        "            # Предсказание\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                pred = model.predict_proba([feature_vector])[0][1]\n",
        "            else:\n",
        "                pred = model.predict([feature_vector])[0]\n",
        "\n",
        "            predictions.append((item_id, pred))\n",
        "\n",
        "        # Сортируем по убыванию score\n",
        "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return predictions[:top_k]\n",
        "\n",
        "    def plot_feature_importance(self, model_type='xgboost'):\n",
        "        \"\"\"Визуализация важности признаков\"\"\"\n",
        "\n",
        "        if model_type not in self.models:\n",
        "            raise ValueError(f\"Model {model_type} not trained yet!\")\n",
        "\n",
        "        model = self.models[model_type]['model']\n",
        "        feature_names = self.models[model_type]['feature_names']\n",
        "\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "\n",
        "            # Сортируем по важности\n",
        "            indices = np.argsort(importances)[::-1]\n",
        "\n",
        "            # Берем топ-20 признаков\n",
        "            top_n = min(20, len(feature_names))\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.title(f\"Top {top_n} Feature Importances ({model_type})\")\n",
        "            plt.bar(range(top_n), importances[indices[:top_n]], align='center')\n",
        "            plt.xticks(range(top_n), [feature_names[i] for i in indices[:top_n]], rotation=90)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Выводим топ-10 признаков\n",
        "            print(f\"\\nTop 10 features for {model_type}:\")\n",
        "            for i in range(min(10, len(feature_names))):\n",
        "                print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "        else:\n",
        "            print(f\"Model {model_type} doesn't have feature_importances_ attribute\")\n",
        "\n",
        "# ============================================\n",
        "# 4. ДЕМОНСТРАЦИЯ РАБОТЫ\n",
        "# ============================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"FEATURE COMBINATION RECOMMENDER SYSTEM DEMO\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Генерация данных\n",
        "    print(\"\\n1. Generating synthetic data...\")\n",
        "    user_df, item_df, interactions_df, sessions_df = generate_synthetic_data(\n",
        "        n_users=200, n_items=100, n_interactions=1000\n",
        "    )\n",
        "\n",
        "    print(f\"   Users: {len(user_df)}\")\n",
        "    print(f\"   Items: {len(item_df)}\")\n",
        "    print(f\"   Interactions: {len(interactions_df)}\")\n",
        "    print(f\"   Sessions: {len(sessions_df)}\")\n",
        "\n",
        "    # Тестирование разных методов комбинации\n",
        "    methods = ['concatenate', 'weighted', 'neural', 'attention']\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for method in methods:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Testing {method.upper()} feature combination method\")\n",
        "        print('='*60)\n",
        "\n",
        "        # Создаем рекомендательную систему\n",
        "        recommender = FeatureCombinationRecommender(combination_method=method)\n",
        "\n",
        "        # Подготавливаем признаки\n",
        "        features = recommender.prepare_features(\n",
        "            user_df, item_df, interactions_df, sessions_df\n",
        "        )\n",
        "\n",
        "        print(f\"\\nSample of combined features ({method}):\")\n",
        "        print(features.head())\n",
        "\n",
        "        # Обучаем модель\n",
        "        model, performance = recommender.train_model(\n",
        "            features, target_column='rating', model_type='random_forest'\n",
        "        )\n",
        "\n",
        "        results[method] = {\n",
        "            'n_features': len(features.columns),\n",
        "            'performance': performance,\n",
        "            'feature_sample': list(features.columns[:10])\n",
        "        }\n",
        "\n",
        "        # Генерация рекомендаций\n",
        "        print(f\"\\nGenerating recommendations with {method} method...\")\n",
        "        recommendations = recommender.recommend(\n",
        "            user_id=1,\n",
        "            item_ids=list(range(1, 21)),\n",
        "            model_type='random_forest',\n",
        "            top_k=5\n",
        "        )\n",
        "\n",
        "        print(\"Top 5 recommendations:\")\n",
        "        for item_id, score in recommendations:\n",
        "            print(f\"  Item {item_id}: score = {score:.4f}\")\n",
        "\n",
        "    # Сравнение методов\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"COMPARISON OF FEATURE COMBINATION METHODS\")\n",
        "    print('='*60)\n",
        "\n",
        "    comparison_df = pd.DataFrame([\n",
        "        {\n",
        "            'Method': method,\n",
        "            'Features': results[method]['n_features'],\n",
        "            'Performance': results[method]['performance'],\n",
        "            'Sample Features': ', '.join(results[method]['feature_sample'][:3])\n",
        "        }\n",
        "        for method in methods\n",
        "    ])\n",
        "\n",
        "    print(comparison_df.to_string(index=False))\n",
        "\n",
        "    # Визуализация сравнения\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(comparison_df['Method'], comparison_df['Features'], color='skyblue')\n",
        "    plt.title('Number of Features by Method')\n",
        "    plt.ylabel('Feature Count')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(comparison_df['Method'], comparison_df['Performance'], color='lightcoral')\n",
        "    plt.title('Model Performance by Method')\n",
        "    plt.ylabel('Performance (Higher is better)')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Дополнительный анализ важности признаков\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Используем лучший метод для анализа\n",
        "    best_method = comparison_df.loc[comparison_df['Performance'].idxmax(), 'Method']\n",
        "    print(f\"\\nAnalyzing feature importance for best method: {best_method}\")\n",
        "\n",
        "    final_recommender = FeatureCombinationRecommender(combination_method=best_method)\n",
        "    final_features = final_recommender.prepare_features(user_df, item_df, interactions_df, sessions_df)\n",
        "    model, _ = final_recommender.train_model(final_features, model_type='random_forest')\n",
        "\n",
        "    # Визуализация важности признаков\n",
        "    final_recommender.plot_feature_importance(model_type='random_forest')\n",
        "\n",
        "    # Пример использования для новых данных\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"PREDICTION EXAMPLE FOR NEW USER-ITEM PAIR\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Создаем пример нового пользователя и товара\n",
        "    new_user_features = {\n",
        "        'user_id': [999],\n",
        "        'age': [30],\n",
        "        'gender': ['M'],\n",
        "        'location': ['NY'],\n",
        "        'join_date': [pd.Timestamp.now()],\n",
        "        'preferred_category': ['tech']\n",
        "    }\n",
        "\n",
        "    new_item_features = {\n",
        "        'item_id': [999],\n",
        "        'category': ['electronics'],\n",
        "        'price': [299.99],\n",
        "        'rating': [4.5],\n",
        "        'reviews_count': [150],\n",
        "        'description': [\"new smartphone with advanced features\"]\n",
        "    }\n",
        "\n",
        "    new_user_df = pd.DataFrame(new_user_features)\n",
        "    new_item_df = pd.DataFrame(new_item_features)\n",
        "\n",
        "    print(\"New user features:\")\n",
        "    print(new_user_df)\n",
        "    print(\"\\nNew item features:\")\n",
        "    print(new_item_df)\n",
        "\n",
        "# Запуск демонстрации\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Tuple, Union\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "\n",
        "# ============================================\n",
        "# 1. ГЕНЕРАЦИЯ СИНТЕТИЧЕСКИХ ДАННЫХ (ИСПРАВЛЕННАЯ)\n",
        "# ============================================\n",
        "\n",
        "def generate_synthetic_data(n_users=1000, n_items=500, n_interactions=5000):\n",
        "    \"\"\"Генерация реалистичных синтетических данных\"\"\"\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Пользователи\n",
        "    users = np.arange(1, n_users + 1)\n",
        "\n",
        "    # Демографические признаки пользователей\n",
        "    user_features = {\n",
        "        'user_id': users,\n",
        "        'age': np.random.randint(18, 65, n_users),\n",
        "        'gender': np.random.choice(['M', 'F'], n_users, p=[0.55, 0.45]),\n",
        "        'location': np.random.choice(['NY', 'LA', 'SF', 'CH', 'MI'], n_users),\n",
        "        'join_date': pd.date_range('2020-01-01', periods=n_users, freq='D'),\n",
        "        'preferred_category': np.random.choice(['tech', 'sports', 'music', 'movies', 'books'], n_users)\n",
        "    }\n",
        "\n",
        "    # Товары\n",
        "    items = np.arange(1, n_items + 1)\n",
        "\n",
        "    # Контентные признаки товаров\n",
        "    categories = ['electronics', 'books', 'clothing', 'home', 'sports', 'beauty']\n",
        "    item_features = {\n",
        "        'item_id': items,\n",
        "        'category': np.random.choice(categories, n_items),\n",
        "        'price': np.random.uniform(5, 500, n_items).round(2),\n",
        "        'rating': np.random.uniform(1, 5, n_items).round(1),\n",
        "        'reviews_count': np.random.randint(0, 1000, n_items),\n",
        "        'description': [f\"product_{i} description with features and benefits\" for i in items]\n",
        "    }\n",
        "\n",
        "    # Взаимодействия пользователь-товар\n",
        "    interactions = []\n",
        "    for _ in range(n_interactions):\n",
        "        user = np.random.choice(users)\n",
        "        item = np.random.choice(items)\n",
        "        rating = np.random.choice([1, 2, 3, 4, 5], p=[0.05, 0.1, 0.2, 0.4, 0.25])\n",
        "        timestamp = pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365))\n",
        "        interactions.append([user, item, rating, timestamp])\n",
        "\n",
        "    interactions_df = pd.DataFrame(interactions, columns=['user_id', 'item_id', 'rating', 'timestamp'])\n",
        "\n",
        "    # Контекстуальные данные (сессии)\n",
        "    sessions = []\n",
        "    for _ in range(min(100, n_interactions // 10)):  # ограничим количество сессий\n",
        "        session_id = np.random.randint(1000, 9999)\n",
        "        user = np.random.choice(users)\n",
        "        session_items = np.random.choice(items, size=np.random.randint(2, 10), replace=False)\n",
        "        for item in session_items:\n",
        "            sessions.append([session_id, user, item])\n",
        "\n",
        "    # Создаем DataFrames\n",
        "    user_df = pd.DataFrame(user_features)\n",
        "    item_df = pd.DataFrame(item_features)\n",
        "    sessions_df = pd.DataFrame(sessions, columns=['session_id', 'user_id', 'item_id']) if sessions else pd.DataFrame()\n",
        "\n",
        "    # Добавляем дополнительные вычисляемые признаки\n",
        "    # Популярность товара\n",
        "    item_popularity = interactions_df.groupby('item_id').size().reset_index(name='popularity')\n",
        "    item_df = item_df.merge(item_popularity, on='item_id', how='left').fillna(0)\n",
        "\n",
        "    # Активность пользователя\n",
        "    user_activity = interactions_df.groupby('user_id').size().reset_index(name='activity_count')\n",
        "    user_df = user_df.merge(user_activity, on='user_id', how='left').fillna(0)\n",
        "\n",
        "    return user_df, item_df, interactions_df, sessions_df\n",
        "\n",
        "# ============================================\n",
        "# 2. ФУНКЦИИ FEATURE EXTRACTION (ИСПРАВЛЕННЫЕ)\n",
        "# ============================================\n",
        "\n",
        "class FeatureExtractor:\n",
        "    \"\"\"Извлечение признаков из разных источников\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=50, stop_words='english')\n",
        "        self.item_svd = TruncatedSVD(n_components=5)\n",
        "        self.user_svd = TruncatedSVD(n_components=5)\n",
        "\n",
        "    def extract_user_features(self, user_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Извлечение признаков пользователей\"\"\"\n",
        "\n",
        "        # Копируем для безопасности\n",
        "        user_features = user_df.copy()\n",
        "\n",
        "        # 1. Демографические признаки (one-hot encoding)\n",
        "        demo_features = pd.get_dummies(\n",
        "            user_features[['gender', 'location', 'preferred_category']],\n",
        "            prefix=['gender', 'loc', 'cat']\n",
        "        )\n",
        "\n",
        "        # 2. Числовые признаки (normalization)\n",
        "        numeric_cols = ['age', 'activity_count']\n",
        "        for col in numeric_cols:\n",
        "            if col in user_features.columns:\n",
        "                user_features[col] = user_features[col].astype(float)\n",
        "\n",
        "        if numeric_cols and all(col in user_features.columns for col in numeric_cols):\n",
        "            user_features[numeric_cols] = self.scaler.fit_transform(user_features[numeric_cols])\n",
        "\n",
        "        # 3. Временные признаки из join_date\n",
        "        if 'join_date' in user_features.columns:\n",
        "            user_features['join_date'] = pd.to_datetime(user_features['join_date'])\n",
        "            user_features['join_year'] = user_features['join_date'].dt.year\n",
        "            user_features['join_month'] = user_features['join_date'].dt.month\n",
        "            user_features['join_day'] = user_features['join_date'].dt.day\n",
        "            user_features['days_since_join'] = (\n",
        "                pd.Timestamp.now() - user_features['join_date']\n",
        "            ).dt.days\n",
        "\n",
        "        # 4. Объединяем все признаки пользователей\n",
        "        numeric_features = ['user_id'] + numeric_cols\n",
        "        if 'join_year' in user_features.columns:\n",
        "            numeric_features.extend(['join_year', 'join_month', 'join_day', 'days_since_join'])\n",
        "\n",
        "        final_features = pd.concat([\n",
        "            user_features[numeric_features],\n",
        "            demo_features\n",
        "        ], axis=1)\n",
        "\n",
        "        return final_features\n",
        "\n",
        "    def extract_item_features(self, item_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Извлечение признаков товаров\"\"\"\n",
        "\n",
        "        item_features = item_df.copy()\n",
        "\n",
        "        # 1. Категориальные признаки (one-hot)\n",
        "        cat_features = pd.get_dummies(\n",
        "            item_features[['category']],\n",
        "            prefix='cat'\n",
        "        )\n",
        "\n",
        "        # 2. Числовые признаки (normalization)\n",
        "        numeric_cols = ['price', 'rating', 'reviews_count', 'popularity']\n",
        "        numeric_cols = [col for col in numeric_cols if col in item_features.columns]\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            item_features[col] = item_features[col].astype(float)\n",
        "\n",
        "        if numeric_cols:\n",
        "            item_features[numeric_cols] = self.scaler.fit_transform(item_features[numeric_cols])\n",
        "\n",
        "        # 3. Текстовые признаки из описания (TF-IDF + SVD)\n",
        "        if 'description' in item_features.columns:\n",
        "            try:\n",
        "                tfidf_matrix = self.tfidf_vectorizer.fit_transform(item_features['description'])\n",
        "                svd_features = self.item_svd.fit_transform(tfidf_matrix)\n",
        "                svd_cols = [f'item_svd_{i}' for i in range(svd_features.shape[1])]\n",
        "                svd_df = pd.DataFrame(svd_features, columns=svd_cols)\n",
        "            except:\n",
        "                svd_df = pd.DataFrame()\n",
        "        else:\n",
        "            svd_df = pd.DataFrame()\n",
        "\n",
        "        # 4. Объединяем все признаки товаров\n",
        "        base_features = item_features[['item_id'] + numeric_cols] if numeric_cols else item_features[['item_id']]\n",
        "\n",
        "        if not svd_df.empty:\n",
        "            final_features = pd.concat([base_features, cat_features, svd_df], axis=1)\n",
        "        else:\n",
        "            final_features = pd.concat([base_features, cat_features], axis=1)\n",
        "\n",
        "        return final_features\n",
        "\n",
        "    def extract_interaction_features(self, interactions_df: pd.DataFrame,\n",
        "                                    user_features: pd.DataFrame,\n",
        "                                    item_features: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Извлечение признаков взаимодействий\"\"\"\n",
        "\n",
        "        interactions = interactions_df.copy()\n",
        "\n",
        "        # 1. Временные признаки\n",
        "        if 'timestamp' in interactions.columns:\n",
        "            interactions['timestamp'] = pd.to_datetime(interactions['timestamp'])\n",
        "            interactions['hour'] = interactions['timestamp'].dt.hour\n",
        "            interactions['day_of_week'] = interactions['timestamp'].dt.dayofweek\n",
        "            interactions['month'] = interactions['timestamp'].dt.month\n",
        "\n",
        "        # 2. Признаки на основе рейтинга\n",
        "        if 'rating' in interactions.columns:\n",
        "            interactions['rating_bin'] = pd.cut(interactions['rating'],\n",
        "                                               bins=[0, 2, 3, 4, 5],\n",
        "                                               labels=['low', 'medium_low', 'medium_high', 'high'])\n",
        "\n",
        "        # 3. One-hot encoding временных признаков (только для существующих колонок)\n",
        "        time_cols_to_encode = []\n",
        "        prefixes = []\n",
        "\n",
        "        if 'hour' in interactions.columns:\n",
        "            time_cols_to_encode.append('hour')\n",
        "            prefixes.append('hour')\n",
        "        if 'day_of_week' in interactions.columns:\n",
        "            time_cols_to_encode.append('day_of_week')\n",
        "            prefixes.append('dow')\n",
        "        if 'month' in interactions.columns:\n",
        "            time_cols_to_encode.append('month')\n",
        "            prefixes.append('month')\n",
        "        if 'rating_bin' in interactions.columns:\n",
        "            time_cols_to_encode.append('rating_bin')\n",
        "            prefixes.append('rating')\n",
        "\n",
        "        if time_cols_to_encode:\n",
        "            time_features = pd.get_dummies(\n",
        "                interactions[time_cols_to_encode],\n",
        "                prefix=prefixes if len(prefixes) == len(time_cols_to_encode) else None\n",
        "            )\n",
        "        else:\n",
        "            time_features = pd.DataFrame()\n",
        "\n",
        "        # 4. Объединяем с признаками пользователей и товаров\n",
        "        combined = interactions.copy()\n",
        "\n",
        "        if 'user_id' in user_features.columns and 'user_id' in combined.columns:\n",
        "            combined = combined.merge(user_features, on='user_id', how='left')\n",
        "\n",
        "        if 'item_id' in item_features.columns and 'item_id' in combined.columns:\n",
        "            combined = combined.merge(item_features, on='item_id', how='left')\n",
        "\n",
        "        # 5. Удаляем исходные столбцы\n",
        "        columns_to_drop = ['timestamp', 'hour', 'day_of_week', 'month', 'rating_bin', 'rating']\n",
        "        columns_to_drop = [col for col in columns_to_drop if col in combined.columns]\n",
        "\n",
        "        if columns_to_drop:\n",
        "            combined = combined.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "        # 6. Объединяем с временными признаками\n",
        "        if not time_features.empty:\n",
        "            combined = pd.concat([combined, time_features], axis=1)\n",
        "\n",
        "        # Заполняем пропущенные значения\n",
        "        combined = combined.fillna(0)\n",
        "\n",
        "        return combined\n",
        "\n",
        "    def extract_session_features(self, sessions_df: pd.DataFrame,\n",
        "                               item_features: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Извлечение сессионных признаков\"\"\"\n",
        "\n",
        "        if sessions_df.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        sessions = sessions_df.copy()\n",
        "\n",
        "        # 1. Агрегация на уровне сессии\n",
        "        session_stats = sessions.groupby('session_id').agg({\n",
        "            'item_id': ['count', lambda x: list(x)],\n",
        "            'user_id': 'first'\n",
        "        }).reset_index()\n",
        "\n",
        "        session_stats.columns = ['session_id', 'session_length', 'item_sequence', 'user_id']\n",
        "\n",
        "        # 2. Признаки сессии\n",
        "        session_features = pd.DataFrame()\n",
        "        session_features['session_id'] = session_stats['session_id']\n",
        "        session_features['session_length'] = session_stats['session_length']\n",
        "\n",
        "        # 3. Средние признаки товаров в сессии\n",
        "        session_item_features = []\n",
        "        for items in session_stats['item_sequence']:\n",
        "            if item_features.empty or 'item_id' not in item_features.columns:\n",
        "                session_item_features.append(np.zeros(5))  # fallback\n",
        "                continue\n",
        "\n",
        "            item_feats = item_features[item_features['item_id'].isin(items)]\n",
        "            if len(item_feats) > 0:\n",
        "                avg_features = item_feats.drop('item_id', axis=1).mean().values\n",
        "            else:\n",
        "                avg_features = np.zeros(item_features.shape[1] - 1)\n",
        "            session_item_features.append(avg_features)\n",
        "\n",
        "        # Создаем DataFrame с признаками товаров сессии\n",
        "        if session_item_features:\n",
        "            session_item_df = pd.DataFrame(\n",
        "                session_item_features,\n",
        "                columns=[f'session_item_{i}' for i in range(len(session_item_features[0]))]\n",
        "            )\n",
        "\n",
        "            # 4. Объединяем все сессионные признаки\n",
        "            final_features = pd.concat([session_features, session_item_df], axis=1)\n",
        "        else:\n",
        "            final_features = session_features\n",
        "\n",
        "        return final_features\n",
        "\n",
        "# ============================================\n",
        "# 3. КЛАСС ДЛЯ FEATURE COMBINATION (УПРОЩЕННЫЙ)\n",
        "# ============================================\n",
        "\n",
        "class FeatureCombinationRecommender:\n",
        "    \"\"\"\n",
        "    Рекомендательная система с комбинацией признаков\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, combination_method='concatenate'):\n",
        "        self.combination_method = combination_method\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.models = {}\n",
        "\n",
        "    def prepare_features(self, user_df, item_df, interactions_df, sessions_df=None):\n",
        "        \"\"\"Подготовка и комбинация всех признаков\"\"\"\n",
        "\n",
        "        print(\"Step 1: Extracting user features...\")\n",
        "        user_features = self.feature_extractor.extract_user_features(user_df)\n",
        "\n",
        "        print(\"Step 2: Extracting item features...\")\n",
        "        item_features = self.feature_extractor.extract_item_features(item_df)\n",
        "\n",
        "        print(\"Step 3: Extracting interaction features...\")\n",
        "        interaction_features = self.feature_extractor.extract_interaction_features(\n",
        "            interactions_df, user_features, item_features\n",
        "        )\n",
        "\n",
        "        # Комбинируем признаки\n",
        "        if self.combination_method == 'concatenate':\n",
        "            combined_features = self._concatenate_features(\n",
        "                interaction_features, user_features, item_features\n",
        "            )\n",
        "        elif self.combination_method == 'weighted':\n",
        "            combined_features = self._weighted_combination(\n",
        "                interaction_features, user_features, item_features\n",
        "            )\n",
        "        elif self.combination_method == 'attention':\n",
        "            combined_features = self._attention_combination(\n",
        "                interaction_features, user_features, item_features\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown combination method: {self.combination_method}\")\n",
        "\n",
        "        # Добавляем сессионные признаки если есть\n",
        "        if sessions_df is not None and not sessions_df.empty:\n",
        "            print(\"Step 4: Extracting session features...\")\n",
        "            session_features = self.feature_extractor.extract_session_features(\n",
        "                sessions_df, item_features\n",
        "            )\n",
        "            if not session_features.empty and 'user_id' in session_features.columns:\n",
        "                combined_features = combined_features.merge(\n",
        "                    session_features, on='user_id', how='left'\n",
        "                ).fillna(0)\n",
        "\n",
        "        # Сохраняем информацию о признаках\n",
        "        self.feature_info = {\n",
        "            'user_features': list(user_features.columns),\n",
        "            'item_features': list(item_features.columns),\n",
        "            'interaction_features': list(interaction_features.columns),\n",
        "            'combined_features': list(combined_features.columns)\n",
        "        }\n",
        "\n",
        "        print(f\"\\nFeature extraction complete!\")\n",
        "        print(f\"  User features: {len(user_features.columns)}\")\n",
        "        print(f\"  Item features: {len(item_features.columns)}\")\n",
        "        print(f\"  Interaction features: {len(interaction_features.columns)}\")\n",
        "        print(f\"  Total combined features: {len(combined_features.columns)}\")\n",
        "\n",
        "        return combined_features\n",
        "\n",
        "    def _concatenate_features(self, interaction_features, user_features, item_features):\n",
        "        \"\"\"Простая конкатенация всех признаков\"\"\"\n",
        "\n",
        "        # Объединяем все признаки\n",
        "        combined = interaction_features.copy()\n",
        "\n",
        "        if 'user_id' in combined.columns:\n",
        "            combined = combined.merge(user_features, on='user_id', how='left')\n",
        "\n",
        "        if 'item_id' in combined.columns:\n",
        "            combined = combined.merge(item_features, on='item_id', how='left')\n",
        "\n",
        "        # Заполняем пропуски\n",
        "        combined = combined.fillna(0)\n",
        "\n",
        "        # Удаляем возможные дубликаты колонок\n",
        "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
        "\n",
        "        return combined\n",
        "\n",
        "    def _weighted_combination(self, interaction_features, user_features, item_features):\n",
        "        \"\"\"Взвешенная комбинация признаков\"\"\"\n",
        "\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        # Получаем признаки отдельно\n",
        "        interaction_only = interaction_features.drop(['user_id', 'item_id'], axis=1, errors='ignore')\n",
        "        user_only = user_features.drop(['user_id'], axis=1, errors='ignore')\n",
        "        item_only = item_features.drop(['item_id'], axis=1, errors='ignore')\n",
        "\n",
        "        # Определяем веса\n",
        "        interaction_weight = 0.4\n",
        "        user_weight = 0.3\n",
        "        item_weight = 0.3\n",
        "\n",
        "        # Нормализуем признаки если они есть\n",
        "        weighted_features = []\n",
        "\n",
        "        for i in range(len(interaction_features)):\n",
        "            user_id = interaction_features.iloc[i]['user_id']\n",
        "            item_id = interaction_features.iloc[i]['item_id']\n",
        "\n",
        "            # Получаем векторы признаков\n",
        "            inter_vec = interaction_only.iloc[i].values if not interaction_only.empty else np.array([])\n",
        "\n",
        "            user_vec = np.array([])\n",
        "            if not user_only.empty and user_id in user_features['user_id'].values:\n",
        "                user_idx = user_features[user_features['user_id'] == user_id].index\n",
        "                if len(user_idx) > 0:\n",
        "                    user_vec = user_only.iloc[user_idx[0]].values\n",
        "\n",
        "            item_vec = np.array([])\n",
        "            if not item_only.empty and item_id in item_features['item_id'].values:\n",
        "                item_idx = item_features[item_features['item_id'] == item_id].index\n",
        "                if len(item_idx) > 0:\n",
        "                    item_vec = item_only.iloc[item_idx[0]].values\n",
        "\n",
        "            # Нормализуем каждый вектор отдельно\n",
        "            if len(inter_vec) > 0:\n",
        "                inter_vec = scaler.fit_transform(inter_vec.reshape(1, -1)).flatten()\n",
        "            if len(user_vec) > 0:\n",
        "                user_vec = scaler.fit_transform(user_vec.reshape(1, -1)).flatten()\n",
        "            if len(item_vec) > 0:\n",
        "                item_vec = scaler.fit_transform(item_vec.reshape(1, -1)).flatten()\n",
        "\n",
        "            # Взвешенная комбинация\n",
        "            if len(inter_vec) > 0 and len(user_vec) > 0 and len(item_vec) > 0:\n",
        "                # Приводим к одинаковой размерности (берем минимальную)\n",
        "                min_len = min(len(inter_vec), len(user_vec), len(item_vec))\n",
        "                combined = (\n",
        "                    inter_vec[:min_len] * interaction_weight +\n",
        "                    user_vec[:min_len] * user_weight +\n",
        "                    item_vec[:min_len] * item_weight\n",
        "                )\n",
        "            else:\n",
        "                # Используем доступные признаки\n",
        "                available_vecs = []\n",
        "                weights = []\n",
        "                if len(inter_vec) > 0:\n",
        "                    available_vecs.append(inter_vec)\n",
        "                    weights.append(interaction_weight)\n",
        "                if len(user_vec) > 0:\n",
        "                    available_vecs.append(user_vec)\n",
        "                    weights.append(user_weight)\n",
        "                if len(item_vec) > 0:\n",
        "                    available_vecs.append(item_vec)\n",
        "                    weights.append(item_weight)\n",
        "\n",
        "                if available_vecs:\n",
        "                    # Нормализуем веса\n",
        "                    total_weight = sum(weights)\n",
        "                    weights = [w/total_weight for w in weights]\n",
        "\n",
        "                    # Находим минимальную длину\n",
        "                    min_len = min(len(v) for v in available_vecs)\n",
        "                    combined = np.zeros(min_len)\n",
        "                    for vec, weight in zip(available_vecs, weights):\n",
        "                        combined += vec[:min_len] * weight\n",
        "                else:\n",
        "                    combined = np.array([0.5])  # fallback\n",
        "\n",
        "            weighted_features.append(combined)\n",
        "\n",
        "        # Создаем DataFrame\n",
        "        max_len = max(len(f) for f in weighted_features)\n",
        "        padded_features = []\n",
        "        for f in weighted_features:\n",
        "            if len(f) < max_len:\n",
        "                f_padded = np.pad(f, (0, max_len - len(f)), mode='constant')\n",
        "            else:\n",
        "                f_padded = f[:max_len]\n",
        "            padded_features.append(f_padded)\n",
        "\n",
        "        weighted_df = pd.DataFrame(\n",
        "            padded_features,\n",
        "            columns=[f'weighted_feat_{i}' for i in range(max_len)]\n",
        "        )\n",
        "\n",
        "        # Добавляем ID\n",
        "        weighted_df['user_id'] = interaction_features['user_id'].values\n",
        "        weighted_df['item_id'] = interaction_features['item_id'].values\n",
        "\n",
        "        return weighted_df\n",
        "\n",
        "    def _attention_combination(self, interaction_features, user_features, item_features):\n",
        "        \"\"\"Комбинация с упрощенным механизмом внимания\"\"\"\n",
        "\n",
        "        print(\"Using simplified attention-based feature combination...\")\n",
        "\n",
        "        # Для простоты используем конкатенацию\n",
        "        combined = self._concatenate_features(interaction_features, user_features, item_features)\n",
        "\n",
        "        # Вычисляем простые статистики для создания attention-like features\n",
        "        numeric_cols = combined.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "        if numeric_cols:\n",
        "            # Вычисляем важность на основе стандартного отклонения\n",
        "            combined['feature_variance'] = combined[numeric_cols].std(axis=1)\n",
        "            combined['feature_mean'] = combined[numeric_cols].mean(axis=1)\n",
        "\n",
        "            # Простой attention weight\n",
        "            combined['attention_weight'] = (\n",
        "                combined['feature_variance'] /\n",
        "                (combined['feature_mean'].abs() + 1e-10)\n",
        "            )\n",
        "\n",
        "            # Нормализуем weight\n",
        "            if combined['attention_weight'].std() > 0:\n",
        "                combined['attention_weight'] = (\n",
        "                    (combined['attention_weight'] - combined['attention_weight'].mean()) /\n",
        "                    combined['attention_weight'].std()\n",
        "                )\n",
        "\n",
        "        return combined\n",
        "\n",
        "    def train_model(self, features, target_column='rating', model_type='random_forest'):\n",
        "        \"\"\"Обучение модели на комбинированных признаках\"\"\"\n",
        "\n",
        "        from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "\n",
        "        print(f\"\\nTraining {model_type} model on combined features...\")\n",
        "\n",
        "        # Подготовка данных\n",
        "        X = features.copy()\n",
        "\n",
        "        # Удаляем ID колонки\n",
        "        id_cols = ['user_id', 'item_id', 'session_id']\n",
        "        id_cols = [col for col in id_cols if col in X.columns]\n",
        "\n",
        "        if id_cols:\n",
        "            X = X.drop(columns=id_cols)\n",
        "\n",
        "        # Проверяем наличие целевой переменной\n",
        "        if target_column in features.columns:\n",
        "            y = features[target_column]\n",
        "            is_classification = False\n",
        "        else:\n",
        "            # Создаем бинарную целевую переменную\n",
        "            y = np.ones(len(features))\n",
        "            is_classification = True\n",
        "\n",
        "        # Проверяем что есть данные для обучения\n",
        "        if len(X) == 0 or len(y) == 0:\n",
        "            print(\"Warning: No data for training!\")\n",
        "            return None, 0\n",
        "\n",
        "        # Разделение на train/test\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y if is_classification else None\n",
        "        )\n",
        "\n",
        "        # Выбор модели\n",
        "        if model_type == 'random_forest':\n",
        "            if is_classification:\n",
        "                model = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=5)\n",
        "                y_train = (y_train > 0).astype(int)\n",
        "                y_test = (y_test > 0).astype(int)\n",
        "            else:\n",
        "                model = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=5)\n",
        "        elif model_type == 'logistic':\n",
        "            model = LogisticRegression(max_iter=500, random_state=42)\n",
        "            y_train = (y_train > 3).astype(int) if not is_classification else y_train\n",
        "            y_test = (y_test > 3).astype(int) if not is_classification else y_test\n",
        "        else:\n",
        "            # Fallback to random forest\n",
        "            model = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=5)\n",
        "\n",
        "        # Обучение\n",
        "        try:\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Предсказание и оценка\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            if is_classification or model_type == 'logistic':\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                print(f\"Accuracy: {accuracy:.4f}\")\n",
        "                metric = accuracy\n",
        "            else:\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                print(f\"MSE: {mse:.4f}\")\n",
        "                metric = mse\n",
        "\n",
        "            # Сохраняем модель\n",
        "            self.models[model_type] = {\n",
        "                'model': model,\n",
        "                'feature_names': list(X.columns),\n",
        "                'performance': metric\n",
        "            }\n",
        "\n",
        "            return model, metric\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training model: {e}\")\n",
        "            return None, 0\n",
        "\n",
        "    def plot_feature_importance(self, model_type='random_forest'):\n",
        "        \"\"\"Визуализация важности признаков\"\"\"\n",
        "\n",
        "        if model_type not in self.models:\n",
        "            print(f\"Model {model_type} not trained yet!\")\n",
        "            return\n",
        "\n",
        "        model = self.models[model_type]['model']\n",
        "        feature_names = self.models[model_type]['feature_names']\n",
        "\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "\n",
        "            if len(importances) == 0:\n",
        "                print(\"No feature importances available\")\n",
        "                return\n",
        "\n",
        "            # Сортируем по важности\n",
        "            indices = np.argsort(importances)[::-1]\n",
        "\n",
        "            # Берем топ-15 признаков\n",
        "            top_n = min(15, len(feature_names), len(importances))\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.title(f\"Top {top_n} Feature Importances ({model_type})\")\n",
        "            bars = plt.bar(range(top_n), importances[indices[:top_n]], align='center', color='skyblue')\n",
        "            plt.xticks(range(top_n), [feature_names[i] for i in indices[:top_n]], rotation=45, ha='right')\n",
        "            plt.xlabel('Features')\n",
        "            plt.ylabel('Importance')\n",
        "\n",
        "            # Добавляем значения на бары\n",
        "            for bar, importance in zip(bars, importances[indices[:top_n]]):\n",
        "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "                        f'{importance:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Выводим топ-5 признаков\n",
        "            print(f\"\\nTop 5 features for {model_type}:\")\n",
        "            for i in range(min(5, len(feature_names), len(importances))):\n",
        "                print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "        else:\n",
        "            print(f\"Model {model_type} doesn't have feature_importances_ attribute\")\n",
        "\n",
        "# ============================================\n",
        "# 4. ДЕМОНСТРАЦИЯ РАБОТЫ\n",
        "# ============================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"FEATURE COMBINATION RECOMMENDER SYSTEM DEMO\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Генерация данных\n",
        "    print(\"\\n1. Generating synthetic data...\")\n",
        "    user_df, item_df, interactions_df, sessions_df = generate_synthetic_data(\n",
        "        n_users=100, n_items=50, n_interactions=500\n",
        "    )\n",
        "\n",
        "    print(f\"   Users: {len(user_df)}\")\n",
        "    print(f\"   Items: {len(item_df)}\")\n",
        "    print(f\"   Interactions: {len(interactions_df)}\")\n",
        "    print(f\"   Sessions: {len(sessions_df)}\")\n",
        "\n",
        "    # Покажем данные\n",
        "    print(\"\\nSample user data:\")\n",
        "    print(user_df.head())\n",
        "\n",
        "    print(\"\\nSample item data:\")\n",
        "    print(item_df.head())\n",
        "\n",
        "    print(\"\\nSample interactions:\")\n",
        "    print(interactions_df.head())\n",
        "\n",
        "    # Тестирование методов комбинации\n",
        "    methods = ['concatenate', 'weighted', 'attention']\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for method in methods:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Testing {method.upper()} feature combination method\")\n",
        "        print('='*60)\n",
        "\n",
        "        # Создаем рекомендательную систему\n",
        "        recommender = FeatureCombinationRecommender(combination_method=method)\n",
        "\n",
        "        # Подготавливаем признаки\n",
        "        features = recommender.prepare_features(\n",
        "            user_df, item_df, interactions_df, sessions_df\n",
        "        )\n",
        "\n",
        "        print(f\"\\nSample of combined features ({method}):\")\n",
        "        print(features.head(3))\n",
        "        print(f\"Shape: {features.shape}\")\n",
        "\n",
        "        # Обучаем модель\n",
        "        if len(features) > 10:  # Минимум данных для обучения\n",
        "            model, performance = recommender.train_model(\n",
        "                features, target_column='rating', model_type='random_forest'\n",
        "            )\n",
        "\n",
        "            if model is not None:\n",
        "                results[method] = {\n",
        "                    'n_features': len(features.columns),\n",
        "                    'performance': performance,\n",
        "                    'n_samples': len(features),\n",
        "                    'feature_sample': list(features.columns[:5])\n",
        "                }\n",
        "\n",
        "                # Визуализация важности признаков\n",
        "                recommender.plot_feature_importance(model_type='random_forest')\n",
        "\n",
        "    # Сравнение методов\n",
        "    if results:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"COMPARISON OF FEATURE COMBINATION METHODS\")\n",
        "        print('='*60)\n",
        "\n",
        "        comparison_data = []\n",
        "        for method in methods:\n",
        "            if method in results:\n",
        "                comparison_data.append({\n",
        "                    'Method': method,\n",
        "                    'Features': results[method]['n_features'],\n",
        "                    'Performance': f\"{results[method]['performance']:.4f}\",\n",
        "                    'Samples': results[method]['n_samples'],\n",
        "                    'Sample Features': ', '.join(results[method]['feature_sample'])\n",
        "                })\n",
        "\n",
        "        if comparison_data:\n",
        "            comparison_df = pd.DataFrame(comparison_data)\n",
        "            print(comparison_df.to_string(index=False))\n",
        "\n",
        "            # Визуализация сравнения\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "            methods_list = [d['Method'] for d in comparison_data]\n",
        "            features_list = [d['Features'] for d in comparison_data]\n",
        "            performance_list = [float(d['Performance']) for d in comparison_data]\n",
        "            samples_list = [d['Samples'] for d in comparison_data]\n",
        "\n",
        "            axes[0].bar(methods_list, features_list, color='skyblue')\n",
        "            axes[0].set_title('Number of Features')\n",
        "            axes[0].set_ylabel('Count')\n",
        "\n",
        "            axes[1].bar(methods_list, performance_list, color='lightcoral')\n",
        "            axes[1].set_title('Model Performance')\n",
        "            axes[1].set_ylabel('Score')\n",
        "\n",
        "            axes[2].bar(methods_list, samples_list, color='lightgreen')\n",
        "            axes[2].set_title('Number of Samples')\n",
        "            axes[2].set_ylabel('Count')\n",
        "\n",
        "            for ax in axes:\n",
        "                ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"DEMO COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Запуск демонстрации\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uBf0zoBVHNjL",
        "outputId": "4952ca60-32a5-4e83-bfc1-156bfbe82d26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful!\n",
            "============================================================\n",
            "FEATURE COMBINATION RECOMMENDER SYSTEM DEMO\n",
            "============================================================\n",
            "\n",
            "1. Generating synthetic data...\n",
            "   Users: 100\n",
            "   Items: 50\n",
            "   Interactions: 500\n",
            "   Sessions: 267\n",
            "\n",
            "Sample user data:\n",
            "   user_id  age gender location  join_date preferred_category  activity_count\n",
            "0        1   56      M       LA 2020-01-01               tech             2.0\n",
            "1        2   46      M       SF 2020-01-02             sports             6.0\n",
            "2        3   32      M       SF 2020-01-03             sports             4.0\n",
            "3        4   60      F       CH 2020-01-04              music             7.0\n",
            "4        5   25      F       CH 2020-01-05             sports            10.0\n",
            "\n",
            "Sample item data:\n",
            "   item_id     category   price  rating  reviews_count  \\\n",
            "0        1         home  343.09     3.0            148   \n",
            "1        2  electronics   40.24     1.2             79   \n",
            "2        3       beauty  162.89     3.2            885   \n",
            "3        4       beauty  423.21     2.8            212   \n",
            "4        5       sports   16.52     4.6            202   \n",
            "\n",
            "                                        description  popularity  \n",
            "0  product_1 description with features and benefits          11  \n",
            "1  product_2 description with features and benefits          10  \n",
            "2  product_3 description with features and benefits          11  \n",
            "3  product_4 description with features and benefits           9  \n",
            "4  product_5 description with features and benefits          11  \n",
            "\n",
            "Sample interactions:\n",
            "   user_id  item_id  rating  timestamp\n",
            "0       41       36       5 2023-05-25\n",
            "1       73       33       4 2023-08-08\n",
            "2       29       13       5 2023-09-10\n",
            "3        6       18       4 2023-07-16\n",
            "4       47       25       4 2023-11-08\n",
            "\n",
            "============================================================\n",
            "Testing CONCATENATE feature combination method\n",
            "============================================================\n",
            "Step 1: Extracting user features...\n",
            "Step 2: Extracting item features...\n",
            "Step 3: Extracting interaction features...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of 'prefix' (4) did not match the length of the columns being encoded (1).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-270623367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;31m# Запуск демонстрации\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-270623367.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;31m# Подготавливаем признаки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m         features = recommender.prepare_features(\n\u001b[0m\u001b[1;32m    712\u001b[0m             \u001b[0muser_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractions_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msessions_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-270623367.py\u001b[0m in \u001b[0;36mprepare_features\u001b[0;34m(self, user_df, item_df, interactions_df, sessions_df)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step 3: Extracting interaction features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         interaction_features = self.feature_extractor.extract_interaction_features(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0minteractions_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-270623367.py\u001b[0m in \u001b[0;36mextract_interaction_features\u001b[0;34m(self, interactions_df, user_features, item_features)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime_cols_to_encode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             time_features = pd.get_dummies(\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0minteractions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_cols_to_encode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefixes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefixes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_cols_to_encode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prefix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prefix_sep\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mcheck_len\u001b[0;34m(item, name)\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;34mf\"({data_to_encode.shape[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     )\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mcheck_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prefix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of 'prefix' (4) did not match the length of the columns being encoded (1)."
          ]
        }
      ]
    }
  ]
}