{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "class BERT4RecDataset(Dataset):\n",
        "    \"\"\"Датасет для BERT4Rec\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, max_len, mask_prob=0.15, num_items=None, mask_token=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequences: список последовательностей взаимодействий\n",
        "            max_len: максимальная длина последовательности\n",
        "            mask_prob: вероятность маскирования\n",
        "            num_items: общее количество предметов\n",
        "            mask_token: токен для маскирования\n",
        "        \"\"\"\n",
        "        self.sequences = sequences\n",
        "        self.max_len = max_len\n",
        "        self.mask_prob = mask_prob\n",
        "        self.num_items = num_items\n",
        "        self.mask_token = mask_token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "\n",
        "        # Обрезка или дополнение последовательности\n",
        "        if len(sequence) > self.max_len:\n",
        "            sequence = sequence[-self.max_len:]\n",
        "        else:\n",
        "            sequence = [0] * (self.max_len - len(sequence)) + sequence\n",
        "\n",
        "        # Создание маскированных версий\n",
        "        masked_sequence = sequence.copy()\n",
        "        labels = [0] * self.max_len  # 0 для игнорирования\n",
        "        mask = [0] * self.max_len\n",
        "\n",
        "        for i in range(self.max_len):\n",
        "            if sequence[i] != 0:  # Не padding\n",
        "                if random.random() < self.mask_prob:\n",
        "                    prob = random.random()\n",
        "                    if prob < 0.8:\n",
        "                        # 80%: заменить на [MASK]\n",
        "                        masked_sequence[i] = self.mask_token\n",
        "                    elif prob < 0.9:\n",
        "                        # 10%: заменить случайным предметом\n",
        "                        # Убедимся, что случайный предмет в допустимом диапазоне\n",
        "                        masked_sequence[i] = random.randint(2, self.num_items + 1)\n",
        "                    # 10%: оставить как есть\n",
        "                    labels[i] = sequence[i]\n",
        "                    mask[i] = 1\n",
        "\n",
        "        return {\n",
        "            'input_seq': torch.LongTensor(masked_sequence),\n",
        "            'labels': torch.LongTensor(labels),\n",
        "            'mask': torch.LongTensor(mask),\n",
        "            'original_seq': torch.LongTensor(sequence)\n",
        "        }\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Позиционное кодирование для Transformer\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class BERT4Rec(nn.Module):\n",
        "    \"\"\"Модель BERT4Rec для рекомендательных систем\"\"\"\n",
        "\n",
        "    def __init__(self, num_items, max_len, d_model=256, nhead=4,\n",
        "                 num_layers=2, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_items: количество уникальных предметов\n",
        "            max_len: максимальная длина последовательности\n",
        "            d_model: размерность эмбеддингов\n",
        "            nhead: количество head'ов в multi-head attention\n",
        "            num_layers: количество слоев Transformer\n",
        "            dropout: вероятность dropout\n",
        "        \"\"\"\n",
        "        super(BERT4Rec, self).__init__()\n",
        "\n",
        "        self.num_items = num_items\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Важно: +2 для padding (0) и mask (1) токенов\n",
        "        # Выходной слой должен предсказывать num_items + 1 (без учета padding)\n",
        "        self.item_emb = nn.Embedding(num_items + 2, d_model, padding_idx=0)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Transformer layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Output layer: предсказываем num_items + 1 (игнорируя padding token 0)\n",
        "        self.output = nn.Linear(d_model, num_items + 1)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Инициализация весов\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Инициализация весов\"\"\"\n",
        "        initrange = 0.1\n",
        "        self.item_emb.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output.bias.data.zero_()\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_seq: тензор формы [batch_size, seq_len]\n",
        "        Returns:\n",
        "            logits: тензор формы [batch_size, seq_len, num_items+1]\n",
        "        \"\"\"\n",
        "        # Получаем эмбеддинги\n",
        "        item_emb = self.item_emb(input_seq)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Добавляем позиционное кодирование\n",
        "        seq_emb = self.pos_encoder(item_emb)\n",
        "\n",
        "        # Применяем dropout\n",
        "        seq_emb = self.dropout(seq_emb)\n",
        "\n",
        "        # Создаем маску для padding\n",
        "        padding_mask = (input_seq == 0)\n",
        "\n",
        "        # Пропускаем через Transformer\n",
        "        transformer_output = self.transformer(\n",
        "            seq_emb,\n",
        "            src_key_padding_mask=padding_mask\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        transformer_output = self.layer_norm(transformer_output)\n",
        "\n",
        "        # Output layer\n",
        "        logits = self.output(transformer_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def predict_next(self, input_seq, k=10):\n",
        "        \"\"\"\n",
        "        Предсказание следующего предмета в последовательности\n",
        "\n",
        "        Args:\n",
        "            input_seq: последовательность предметов\n",
        "            k: количество предметов для рекомендации\n",
        "        Returns:\n",
        "            top_k_items: топ-k рекомендованных предметов\n",
        "            top_k_probs: их вероятности\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Подготовка входных данных\n",
        "            if len(input_seq) > self.max_len:\n",
        "                input_seq = input_seq[-self.max_len:]\n",
        "            else:\n",
        "                input_seq = [0] * (self.max_len - len(input_seq)) + input_seq\n",
        "\n",
        "            input_tensor = torch.LongTensor(input_seq).unsqueeze(0).to(next(self.parameters()).device)\n",
        "\n",
        "            # Получаем предсказания\n",
        "            logits = self.forward(input_tensor)\n",
        "\n",
        "            # Берем предсказание для последней позиции\n",
        "            last_logits = logits[0, -1, :]\n",
        "            probs = F.softmax(last_logits, dim=-1)\n",
        "\n",
        "            # Получаем топ-k предметов\n",
        "            # +1 потому что output layer не включает padding token (0)\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, k)\n",
        "\n",
        "            return top_k_indices.cpu().numpy(), top_k_probs.cpu().numpy()\n",
        "\n",
        "\n",
        "class BERT4RecTrainer:\n",
        "    \"\"\"Тренер для модели BERT4Rec\"\"\"\n",
        "\n",
        "    def __init__(self, model, train_loader, val_loader, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        # Игнорируем padding token (0) в loss\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    def train(self, num_epochs=50, lr=0.001):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            train_loss = 0\n",
        "            train_steps = 0\n",
        "\n",
        "            pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "            for batch in pbar:\n",
        "                input_seq = batch['input_seq'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "                mask = batch['mask'].to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.model(input_seq)\n",
        "\n",
        "                # Вычисляем loss только для замаскированных позиций\n",
        "                batch_size, seq_len, num_preds = logits.shape\n",
        "                logits = logits.view(-1, num_preds)\n",
        "                labels = labels.view(-1)\n",
        "                mask = mask.view(-1)\n",
        "\n",
        "                # Применяем маску\n",
        "                masked_logits = logits[mask == 1]\n",
        "                masked_labels = labels[mask == 1]\n",
        "\n",
        "                if len(masked_labels) > 0:\n",
        "                    # Важно: уменьшаем labels на 1, потому что выходной слой\n",
        "                    # предсказывает num_items+1, начиная с 0\n",
        "                    # и padding token (0) игнорируется\n",
        "                    loss_labels = masked_labels - 1\n",
        "\n",
        "                    # Проверяем границы\n",
        "                    if (loss_labels >= 0).all() and (loss_labels < num_preds).all():\n",
        "                        loss = self.criterion(masked_logits, loss_labels)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        train_loss += loss.item()\n",
        "                        train_steps += 1\n",
        "\n",
        "                pbar.set_postfix({'loss': loss.item() if 'loss' in locals() else 0})\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_metrics = self.evaluate()\n",
        "\n",
        "            # Сохраняем лучшую модель\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), 'bert4rec_best.pth')\n",
        "\n",
        "            print(f'Epoch {epoch+1}: Train Loss: {train_loss/train_steps:.4f}, '\n",
        "                  f'Val Loss: {val_loss:.4f}, '\n",
        "                  f'Val Metrics: {val_metrics}')\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        val_loss = 0\n",
        "        val_steps = 0\n",
        "\n",
        "        # Метрики\n",
        "        hits_at_10 = 0\n",
        "        ndcg_at_10 = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                input_seq = batch['input_seq'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "                mask = batch['mask'].to(self.device)\n",
        "                original_seq = batch['original_seq'].to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.model(input_seq)\n",
        "\n",
        "                # Вычисляем loss\n",
        "                batch_size, seq_len, num_preds = logits.shape\n",
        "                logits = logits.view(-1, num_preds)\n",
        "                labels = labels.view(-1)\n",
        "                mask = mask.view(-1)\n",
        "\n",
        "                masked_logits = logits[mask == 1]\n",
        "                masked_labels = labels[mask == 1]\n",
        "\n",
        "                if len(masked_labels) > 0:\n",
        "                    loss_labels = masked_labels - 1\n",
        "                    if (loss_labels >= 0).all() and (loss_labels < num_preds).all():\n",
        "                        loss = self.criterion(masked_logits, loss_labels)\n",
        "                        val_loss += loss.item()\n",
        "                        val_steps += 1\n",
        "\n",
        "                # Вычисляем метрики для последней позиции\n",
        "                for i in range(batch_size):\n",
        "                    # Берем последний ненулевой элемент из оригинальной последовательности\n",
        "                    seq = original_seq[i].cpu().numpy()\n",
        "                    non_zero_idx = np.where(seq != 0)[0]\n",
        "                    if len(non_zero_idx) > 0:\n",
        "                        last_item = seq[non_zero_idx[-1]]\n",
        "\n",
        "                        # Получаем предсказания для последней позиции\n",
        "                        last_logits = logits[i * seq_len + (seq_len - 1):(i + 1) * seq_len]\n",
        "                        probs = F.softmax(last_logits, dim=-1)\n",
        "\n",
        "                        # Топ-10 предсказаний\n",
        "                        top_10 = torch.topk(probs, min(10, len(probs))).indices.cpu().numpy()\n",
        "\n",
        "                        # Преобразуем обратно к исходным индексам (+1)\n",
        "                        top_10_items = top_10 + 1\n",
        "\n",
        "                        # Hit Rate @ 10\n",
        "                        if last_item in top_10_items:\n",
        "                            hits_at_10 += 1\n",
        "\n",
        "                            # NDCG @ 10\n",
        "                            rank = np.where(top_10_items == last_item)[0][0] + 1\n",
        "                            ndcg_at_10 += 1 / np.log2(rank + 1)\n",
        "\n",
        "                        total_samples += 1\n",
        "\n",
        "        metrics = {\n",
        "            'HR@10': hits_at_10 / max(total_samples, 1),\n",
        "            'NDCG@10': ndcg_at_10 / max(total_samples, 1)\n",
        "        }\n",
        "\n",
        "        return val_loss / max(val_steps, 1), metrics\n",
        "\n",
        "\n",
        "def prepare_data(ratings, min_interactions=5, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Подготовка данных для BERT4Rec\n",
        "\n",
        "    Args:\n",
        "        ratings: DataFrame с колонками ['user_id', 'item_id', 'timestamp']\n",
        "        min_interactions: минимальное количество взаимодействий для пользователя\n",
        "        test_size: доля тестовых данных\n",
        "    \"\"\"\n",
        "    # Фильтрация пользователей\n",
        "    user_counts = ratings['user_id'].value_counts()\n",
        "    valid_users = user_counts[user_counts >= min_interactions].index\n",
        "    ratings = ratings[ratings['user_id'].isin(valid_users)]\n",
        "\n",
        "    # Сортировка по времени\n",
        "    ratings = ratings.sort_values(['user_id', 'timestamp'])\n",
        "\n",
        "    # Создание последовательностей\n",
        "    sequences = []\n",
        "    for user_id, group in ratings.groupby('user_id'):\n",
        "        seq = group['item_id'].tolist()\n",
        "        sequences.append(seq)\n",
        "\n",
        "    # Разделение на train/val\n",
        "    train_seq, val_seq = train_test_split(sequences, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Создание словаря предметов\n",
        "    all_items = set()\n",
        "    for seq in sequences:\n",
        "        all_items.update(seq)\n",
        "    num_items = len(all_items)\n",
        "\n",
        "    # Перенумерация предметов:\n",
        "    # 0 - padding token\n",
        "    # 1 - mask token\n",
        "    # 2+ - реальные предметы\n",
        "    item_to_idx = {item: i+2 for i, item in enumerate(all_items)}\n",
        "    idx_to_item = {i+2: item for i, item in enumerate(all_items)}\n",
        "\n",
        "    # Преобразование последовательностей\n",
        "    train_seq_idx = [[item_to_idx[item] for item in seq] for seq in train_seq]\n",
        "    val_seq_idx = [[item_to_idx[item] for item in seq] for seq in val_seq]\n",
        "\n",
        "    return train_seq_idx, val_seq_idx, num_items, idx_to_item\n",
        "\n",
        "\n",
        "def create_synthetic_data(num_users=1000, num_items=500, num_interactions=20000):\n",
        "    \"\"\"Создание синтетических данных для демонстрации\"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Генерация данных\n",
        "    ratings_data = {\n",
        "        'user_id': np.random.randint(1, num_users+1, num_interactions),\n",
        "        'item_id': np.random.randint(1, num_items+1, num_interactions),\n",
        "        'timestamp': np.arange(num_interactions)\n",
        "    }\n",
        "\n",
        "    # Добавим некоторые паттерны для реалистичности\n",
        "    for i in range(num_interactions):\n",
        "        # Некоторые пользователи предпочитают определенные предметы\n",
        "        if ratings_data['user_id'][i] % 10 == 0:\n",
        "            ratings_data['item_id'][i] = np.random.randint(1, 51)\n",
        "\n",
        "    return pd.DataFrame(ratings_data)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Пример использования BERT4Rec\"\"\"\n",
        "\n",
        "    # Параметры\n",
        "    MAX_LEN = 20  # Уменьшим для быстрого тестирования\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_EPOCHS = 5  # Уменьшим для демонстрации\n",
        "    D_MODEL = 64  # Уменьшим размерность\n",
        "    NHEAD = 2  # Уменьшим количество head'ов\n",
        "    NUM_LAYERS = 1  # Уменьшим количество слоев\n",
        "    DROPOUT = 0.1\n",
        "    MASK_PROB = 0.2\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    print(f\"Используется устройство: {DEVICE}\")\n",
        "\n",
        "    # Создание синтетических данных\n",
        "    print(\"Создание синтетических данных...\")\n",
        "    ratings_df = create_synthetic_data(\n",
        "        num_users=200,  # Уменьшим для быстрого тестирования\n",
        "        num_items=100,\n",
        "        num_interactions=5000\n",
        "    )\n",
        "\n",
        "    # Подготовка данных\n",
        "    print(\"Подготовка данных...\")\n",
        "    train_seq, val_seq, num_items, idx_to_item = prepare_data(\n",
        "        ratings_df, min_interactions=3, test_size=0.1\n",
        "    )\n",
        "\n",
        "    print(f\"Количество train последовательностей: {len(train_seq)}\")\n",
        "    print(f\"Количество val последовательностей: {len(val_seq)}\")\n",
        "    print(f\"Количество уникальных предметов: {num_items}\")\n",
        "    print(f\"Длина последовательности: {MAX_LEN}\")\n",
        "\n",
        "    # Создание датасетов и загрузчиков данных\n",
        "    train_dataset = BERT4RecDataset(\n",
        "        train_seq,\n",
        "        max_len=MAX_LEN,\n",
        "        mask_prob=MASK_PROB,\n",
        "        num_items=num_items,\n",
        "        mask_token=1  # Токен для маскирования\n",
        "    )\n",
        "\n",
        "    val_dataset = BERT4RecDataset(\n",
        "        val_seq,\n",
        "        max_len=MAX_LEN,\n",
        "        mask_prob=MASK_PROB,\n",
        "        num_items=num_items,\n",
        "        mask_token=1\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    # Создание модели\n",
        "    print(\"Создание модели...\")\n",
        "    model = BERT4Rec(\n",
        "        num_items=num_items,\n",
        "        max_len=MAX_LEN,\n",
        "        d_model=D_MODEL,\n",
        "        nhead=NHEAD,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "\n",
        "    print(f\"Модель создана. Параметров: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Обучение\n",
        "    print(\"Начало обучения...\")\n",
        "    trainer = BERT4RecTrainer(model, train_loader, val_loader, device=DEVICE)\n",
        "    trainer.train(num_epochs=NUM_EPOCHS, lr=0.001)\n",
        "\n",
        "    # Пример предсказания\n",
        "    print(\"\\nПример предсказания...\")\n",
        "    if len(train_seq) > 0:\n",
        "        test_sequence = train_seq[0][-5:]  # Берем последние 5 взаимодействий\n",
        "        print(f\"История пользователя: {[idx_to_item[idx] for idx in test_sequence if idx != 0]}\")\n",
        "\n",
        "        try:\n",
        "            model.load_state_dict(torch.load('bert4rec_best.pth', map_location=DEVICE))\n",
        "            top_items, top_probs = model.predict_next(test_sequence, k=5)\n",
        "            print(\"Топ-5 рекомендаций:\")\n",
        "            for item_idx, prob in zip(top_items, top_probs):\n",
        "                # Преобразуем обратно к исходным индексам\n",
        "                if item_idx + 1 in idx_to_item:\n",
        "                    print(f\"  Предмет {idx_to_item[item_idx + 1]}: вероятность {prob:.4f}\")\n",
        "                else:\n",
        "                    print(f\"  Предмет {item_idx + 1}: вероятность {prob:.4f}\")\n",
        "        except:\n",
        "            print(\"Не удалось загрузить модель для предсказания\")\n",
        "\n",
        "    print(\"\\nОбучение завершено!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWT_hTk5Ev25",
        "outputId": "36d6e1e9-ad37-458b-83aa-eb5a0b099113"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используется устройство: cpu\n",
            "Создание синтетических данных...\n",
            "Подготовка данных...\n",
            "Количество train последовательностей: 180\n",
            "Количество val последовательностей: 20\n",
            "Количество уникальных предметов: 100\n",
            "Длина последовательности: 20\n",
            "Создание модели...\n",
            "Модель создана. Параметров: 63,205\n",
            "Начало обучения...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 6/6 [00:00<00:00, 13.60it/s, loss=4.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 4.7000, Val Loss: 4.6078, Val Metrics: {'HR@10': 0.05, 'NDCG@10': np.float64(0.05)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 6/6 [00:00<00:00, 17.08it/s, loss=4.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 4.6939, Val Loss: 4.6132, Val Metrics: {'HR@10': 0.05, 'NDCG@10': np.float64(0.05)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 6/6 [00:00<00:00, 15.64it/s, loss=4.63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 4.6562, Val Loss: 4.6829, Val Metrics: {'HR@10': 0.05, 'NDCG@10': np.float64(0.05)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 6/6 [00:00<00:00, 19.30it/s, loss=4.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 4.6486, Val Loss: 4.6575, Val Metrics: {'HR@10': 0.05, 'NDCG@10': np.float64(0.05)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 6/6 [00:00<00:00, 24.48it/s, loss=4.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 4.6287, Val Loss: 4.6720, Val Metrics: {'HR@10': 0.05, 'NDCG@10': np.float64(0.05)}\n",
            "\n",
            "Пример предсказания...\n",
            "История пользователя: [14, 3, 80, 14, 92]\n",
            "Топ-5 рекомендаций:\n",
            "  Предмет 42: вероятность 0.0252\n",
            "  Предмет 22: вероятность 0.0226\n",
            "  Предмет 7: вероятность 0.0216\n",
            "  Предмет 18: вероятность 0.0202\n",
            "  Предмет 58: вероятность 0.0187\n",
            "\n",
            "Обучение завершено!\n"
          ]
        }
      ]
    }
  ]
}